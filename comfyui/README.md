# Using MAGI-1 in ComfyUI

## Installation

* Manually [download and install ComfyUI](https://github.com/comfyanonymous/ComfyUI?tab=readme-ov-file#manual-install-windows-linux)

* Download this repository into the path *ComfyUI/custom\_nodes/MAGI-1* and [install the required dependencies](https://github.com/SandAI-org/MAGI-1?tab=readme-ov-file#environment-preparation).

  > ⚠️ To make ComfyUI recognize the custom node, you need to move `comfyui/__init__.py` to the root directory of MAGI-1.

* Download the MAGI-1 model files to your local machine. In the MAGI-1 config file, such as `example/4.5B/4.5B_base_config.json` (if using the 4.5B base model), change the model weights paths to absolute local paths. The following three fields need to be updated:

  * **load**: Absolute path to the DiT model weights
  * **t5\_pretrained**: Absolute path to the T5 model weights
  * **vae\_pretrained**: Absolute path to the VAE model weights

## Node Functions

After installation, start ComfyUI from the ComfyUI directory:

```shell
cd ComfyUI
# If comfy-cli is installed
comfy launch
# Otherwise
python main.py
```

You can find the nodes provided by this repository under *Add Node - Magi* in ComfyUI. In newer versions of ComfyUI, the nodes are also available in the NODE LIBRARY on the left.

### Load Prompt

Loads a prompt text from the input for subsequent text encoding processing.

* **prompt**: The input text from the user, supports multiline input.

### T5 Text Encoder

Encodes the prompt into text features (Conditioning Embedding) used for video generation.

* **prompt**: Description text input.
* **t5\_pretrained\_path**: Absolute path to the T5 model weights, pointing to the pretrained model in the `ckpt/t5` directory.
* **t5\_device**: Specifies the device on which the T5 model is loaded and run, options include `"cpu"` or `"cuda:x"` (e.g., `"cuda:0"`).

### Load Image

Loads an image file from the input directory. File picker is supported for image uploads.

* **image\_path**: Select image file from ComfyUI's input folder. The system automatically filters out non-image file types and displays only supported formats.

### Process with MAGI

The core node for text-to-video, image-to-video, or video continuation tasks. It generates a video sequence and passes the frame rate to downstream video-saving nodes.

* **task\_mode**: Specifies which task to perform among *text-to-video, image-to-video, or video continuation*.
* **config\_path**: Absolute path to the JSON configuration file required for model execution.

  > ⚠️ All paths in the config file must also be absolute.
* **image\_path**: Absolute path of the image or video to be converted into a video.
* **text\_embeddings**: The embeddings and masks generated by the text encoder, serving as semantic guidance for video generation.
* **magi\_seed**: Random seed to control reproducibility. Using the same seed will yield the same output. Default is 1234, valid range is 0–100000.
* **video\_size\_h**: Height (in pixels) of the generated video. Default is 720. Large values may slow down processing or cause out-of-memory issues—use with caution.
* **video\_size\_w**: Width (in pixels) of the generated video. Default is 720. Same caution as above.
* **num\_frames**: Total number of video frames, determines video duration. Default is 96, range is 24–24000.
* **num\_steps**: Number of diffusion sampling steps. More steps lead to better quality but longer inference time. Default is 64, range is 4–240.
* **fps**: Frame rate (frames per second) of the generated video. Affects playback speed and smoothness. Default is 24, supported range is 1–60.

> ⚠️ This node sets a series of distributed and memory-related environment variables before running.

### Save Video

Saves the generated video sequence to a local file.

* **video**: The video tensor to be saved, essentially a `torch.Tensor`.
* **output\_path**: Absolute path to save the video file (only `.mp4` extension is supported).
* **fps**: Video frame rate. Default is 24. Supports integer values from 1 to 60.

The video will be encoded using the specified frame rate and saved to the `output_path`.

## Workflow Examples

This section provides example workflows for image-to-video generation. You can import workflows via the *Load* button in the menu. In newer versions of ComfyUI, use *Workflow - Open* from the top-left menu, or copy the workflow files to the `user/default/workflows` directory of ComfyUI and refresh the *Workflow* panel on the left to access them.

Workflows are located in the `comfyui/workflow/` directory. Assets are in the `example/assets/` directory.

After importing a workflow, **you must manually reassign the corresponding file paths.**

### Text-to-Video

Workflow file: `workflow/magi_text_to_video_example.json`

### Image-to-Video

Workflow file: `workflow/magi_image_to_video_example.json`

### Video Continuation

Workflow file: `workflow/magi_video_continuation_example.json`
